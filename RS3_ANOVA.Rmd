---
title: "RS3_ANOVA"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load packages}
# rm(list = ls())
library(ggplot2)
library(tidyr)
library(dplyr)
library(car) # anova and levenes table
library(apaTables) # for apa tables
library(MBESS) # for apa tables
library(WRS2) # for non-parametric
library(e1071) # for skewness calculation
library(ggpubr) # for interaction plots/boxplots
library(effsize) # for calculating effect sizes
library(compute.es) # ancova using type 3 method
library(multcomp) # post hoc for ancova type 3 method
library(lmPerm)
library(lme4)
library(afex) # analysis of factorial experiments
library(lsr) # for calculating partial eta squared

# example dataframe for stack overflow
# dput(head(modellingdata, n = 5))
```

```{r loading in data}

# home computer working directory
# setwd("C:/Users/Courtney/Documents/PhD/Teaching/teachingcode")

# work computer working directory
setwd("C:/Users/pscmgo/OneDrive for Business/PhD/Project/Experiment_Code/teachingcode")
temp = list.files(pattern = c("DataSet_2019", "*.csv")) # list all CSV files in the directory
myfiles = lapply(temp, read.csv) # read these CSV in the directory
workingdata <- do.call(rbind.data.frame, myfiles) # convert and combine the CSV files into dataframe

```

```{r demographics, normality mean scores and reshaping data}

# computing histograms and normality curves for each condition
ggplot(workingdata %>%
         gather("NoAS_Ver", "NoAS_VerDemo", "AS_Ver", "AS_VerDemo", key = "condition", value = "correctpairs"), aes(x = correctpairs)) +
  geom_histogram(aes(y = ..density..)) +
  stat_function(
		fun = dnorm, 
		args = with(workingdata %>%
		              gather("NoAS_Ver", "NoAS_VerDemo", "AS_Ver", "AS_VerDemo", key = "condition", value = "correctpairs"), c(mean = mean(correctpairs), sd = sd(correctpairs)))) + 
  facet_wrap( ~ condition)

# spliting conditions into factors and their levels
split_data <- workingdata %>%
  gather("NoAS_Ver", "NoAS_VerDemo", "AS_Ver", "AS_VerDemo", key = "condition", value = "correctpairs") %>%
  separate(condition, into = c("task", "presentation"))

# age demographics
range(split_data$Age)
mean(split_data$Age)
sd(split_data$Age)

# mean scores for condition
split_data %>%
  group_by(task, presentation) %>%
  summarise(mean_corrected_pairs = mean(correctpairs))

```

Histograms show that data seems fairly normally distributed for each condition. Mean values for each condition show differences. 

```{r checking normality of the data}

# residuals versus fitted values
plot(standard.two.aov, 1)

# levenes test
leveneTest(correctpairs ~ task * presentation, data = split_data)

# normality Q-Q plot
plot(standard.two.aov, 2)

```

Homogeneity of variance is an assumption used for t-tests and ANOVAs whereby all comparison groups have the same variance. If this is assumption is violated, it can inflate the t and f values. Homogeneity is not assumed, so significance of these tests indicates a violation. 

T and F avlues are fairly robust against violations so long as the group sizes are similar. If group sizes are vastly unequal and homogeneity of variance is violated, then the F statistic will be biased when large sample variances are associated with small group sizes.  When this occurs, the significance level will be underestimated, which can cause the null hypothesis to be falsely rejected. 

**Residuals versus fitted**
Shows dependency of residuals to fitted values to see if the variance is consistent across the values. This plots indicates that this is not the case. 

**Levene's test**
Levene's test just reaches significance. However as we have equal groups within our condition, our test should be robust against this violation. 

**Normality Q-Q plot**
This plots quantiles from your residuals against quantiles from a normal distribution. A 45 degree line indicates what normality of the data would look like. As we can see, this data would look normally distributed as most points fall on the line.

```{r standard two-way anova}

options(contrasts = c("contr.sum", "contr.poly")) # some documentaion says this is the correct options for type 3 sum of squares

options(contrasts = c("contr.helmert", "contr.poly")) # apaTables documentation says this is the correct option for matching SPSS output

standard.two.aov <- aov(correctpairs ~ task * presentation, data = split_data)

Anova(standard.two.aov, type = 3)

etaSquared(standard.two.aov, type = 3, anova = TRUE)

```

Despite doing the same analysis, R and SPSS produce different outpout for the repeated measures ANOVA. 

Type 1 sum of squares performs analyses sequentially i.e. how they appears within the model that you specify.

Type 3 sum of squares performs analyses in light of other main effects and interactions.

The distinction is important, because the sometimes a main effect can be generated BECAUSE of the an interaction (i.e. on factor level has a low average because of the interaction). Thus is you use type 1 or 3 in this instance, you are likely to get different results.

```{r means and anova with table output in word document}

# apa table ANOVA
setwd("C:/Users/pscmgo/OneDrive for Business/PhD/Project/Experiment_Code/teachingcode")
apa.1way.table(condition, correctpairs, workingdata, filename = "RS3_seminar.doc", show.conf.interval = TRUE, landscape = TRUE)

result <- aov(correctpairs ~ condition, data = workingdata)
summary(result)

# Post-hoc tests (only to be used on anova1 type of anova)
resultTukey <-TukeyHSD(result)
resultTukey




```