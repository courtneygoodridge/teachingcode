---
title: "RS3_ANOVA"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load packages}
# rm(list = ls())
library(ggplot2) # data vis
library(tidyr) # data manipulation
library(dplyr) # data manipulation
library(car) # anova and levenes table
library(apaTables) # for apa tables
library(MBESS) # for apa tables
library(WRS2) # for non-parametric
library(e1071) # for skewness calculation
library(ggpubr) # for interaction plots/boxplots
library(effsize) # for calculating effect sizes
library(compute.es) # ancova using type 3 method
library(multcomp) # post hoc for ancova type 3 method
library(lmPerm) # permutation tests for linear models
library(lme4) # modelling
library(afex) # analysis of factorial experiments
library(lsr) # for calculating partial eta squared

# example dataframe for stack overflow
# dput(head(modellingdata, n = 5))

```

```{r loading in data}

# home computer working directory
setwd("C:/Users/Courtney/Documents/PhD/Teaching/teachingcode")

# work computer working directory
# setwd("C:/Users/pscmgo/OneDrive for Business/PhD/Project/Experiment_Code/teachingcode")
temp = list.files(pattern = c("DataSet_2019", "*.csv")) # list all CSV files in the directory
myfiles = lapply(temp, read.csv) # read these CSV in the directory
workingdata <- do.call(rbind.data.frame, myfiles) # convert and combine the CSV files into dataframe

```

```{r demographics, normality mean scores and reshaping data}

workingdata <- workingdata %>%
  dplyr::mutate(ppid = row_number())

# computing histograms and normality curves for each condition
ggplot(workingdata %>%
         gather("NoAS_Ver", "NoAS_VerDemo", "AS_Ver", "AS_VerDemo", key = "condition", value = "correctpairs"), aes(x = correctpairs)) +
  geom_histogram(aes(y = ..density..)) +
  stat_function(
		fun = dnorm, 
		args = with(workingdata %>%
		              gather("NoAS_Ver", "NoAS_VerDemo", "AS_Ver", "AS_VerDemo", key = "condition", value = "correctpairs"), c(mean = mean(correctpairs), sd = sd(correctpairs)))) + 
  facet_wrap( ~ condition)

# spliting conditions into factors and their levels
split_data <- workingdata %>%
  gather("NoAS_Ver", "NoAS_VerDemo", "AS_Ver", "AS_VerDemo", key = "condition", value = "correctpairs") %>%
  separate(condition, into = c("task", "presentation"))

# age demographics
range(split_data$Age)
mean(split_data$Age)
sd(split_data$Age)

# mean scores for condition
split_data %>%
  group_by(task, presentation) %>%
  summarise(mean_corrected_pairs = mean(correctpairs))

```

Histograms show that data seems fairly normally distributed for each condition. Mean values for each condition show differences. 

```{r checking normality of the data}

# residuals versus fitted values
plot(standard.two.aov, 1)

# levenes test
leveneTest(correctpairs ~ task * presentation, data = split_data)

# normality Q-Q plot
plot(standard.two.aov, 2)

```

Homogeneity of variance is an assumption used for t-tests and ANOVAs whereby all comparison groups have the same variance. If this is assumption is violated, it can inflate the t and f values. Homogeneity is not assumed, so significance of these tests indicates a violation. 

T and F avlues are fairly robust against violations so long as the group sizes are similar. If group sizes are vastly unequal and homogeneity of variance is violated, then the F statistic will be biased when large sample variances are associated with small group sizes.  When this occurs, the significance level will be underestimated, which can cause the null hypothesis to be falsely rejected. 

**Residuals versus fitted**
Shows dependency of residuals to fitted values to see if the variance is consistent across the values. This plots indicates that this is not the case. 

**Levene's test**
Levene's test just reaches significance. However as we have equal groups within our condition, our test should be robust against this violation. 

**Normality Q-Q plot**
This plots quantiles from your residuals against quantiles from a normal distribution. A 45 degree line indicates what normality of the data would look like. As we can see, this data would look normally distributed as most points fall on the line.

```{r standard two-way anova}

# sets our factors as a factor data type
split_data$task <- as.factor(split_data$task)
split_data$presentation <- as.factor(split_data$presentation)

options(contrasts = c("contr.sum", "contr.poly"))

standard.two.aov <- aov(correctpairs ~ task * presentation, data = split_data)

Anova(standard.two.aov, type = 3)

etaSquared(standard.two.aov, type = 3, anova = TRUE)

```

This repeated measures ANOVA in R sums of squares, mean square and eta squared values that are the same as SPSS and JASP. However the f value is different and I cannot understand why.

Type 1 sum of squares performs analyses sequentially i.e. how they appears within the model that you specify.

Type 3 sum of squares performs analyses in light of other main effects and interactions.

The distinction is important, because the sometimes a main effect can be generated BECAUSE of the an interaction (i.e. on factor level has a low average because of the interaction). Thus is you use type 1 or 3 in this instance, you are likely to get different results.

**TO DO**

- ASK ON STATS EXCHANGE: Why am I getting values that match JASP on everything except for my f value?
- Understand why "options(contrasts = c("contr.sum", "contr.poly"))" works (check previously sent material on email) 
- Compute interaction plots for ANOVA **done**
- Compute estimated margin means for main effects **done**
- Compute paired samples t test for investigating interaction **done**
- Check values for all these tests match JASP/SPSS
- Check how to compute effect sizes for Tukey post hoc analyses

```{r interaction plot}

ggplot(data = split_data, aes(x = presentation, color = task, group = task, y = correctpairs)) +
         stat_summary(fun.y = mean, geom = "point") +
         stat_summary(fun.y = mean, geom = "line")

```

Interaction plot visualises interaction i.e. a difference in differences.

```{r exploring main effect and interaction}

model.tables(standard.two.aov, "mean", se = TRUE)

TukeyHSD(standard.two.aov)

```

model.tables function computes estimated marginal means for levels wihtin each main effect (they match to SPSS and JASP output). Hence this helps understand the significnat main effects

TukeyHSD function compute post hoc analyses to understand the interaction. Of note are the following comparisons:

**NoAS:Ver-AS:Ver**, **NoAS:VerDemo-AS:VerDemo**

These demonstrate the same mean differences as the two paired samples t-tests in the SPSS output. Hence this is the equivalent in R form. Only problem now is computing effect sizes for the Tukey post hoc analyses.


```{r means and anova with table output in word document}

# apa table ANOVA
setwd("C:/Users/pscmgo/OneDrive for Business/PhD/Project/Experiment_Code/teachingcode")
apa.1way.table(condition, correctpairs, workingdata, filename = "RS3_seminar.doc", show.conf.interval = TRUE, landscape = TRUE)

```